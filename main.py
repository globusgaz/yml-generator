import os
import requests
from lxml import etree
from datetime import datetime

FEEDS_FILE = "feeds.txt"
MAX_FILE_SIZE_MB = 100
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}

def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]

def fetch_offers_from_url(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        tree = etree.fromstring(response.content)
        offers = tree.findall(".//offer")
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {url} ‚Äî –∑–Ω–∞–π–¥–µ–Ω–æ {len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤")
        return offers
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {url}: {e}")
        return []

def build_prom_yml(offers):
    yml_catalog = etree.Element("yml_catalog", date=datetime.now().strftime("%Y-%m-%d %H:%M"))
    shop = etree.SubElement(yml_catalog, "shop")

    etree.SubElement(shop, "name").text = "MyShop"
    etree.SubElement(shop, "company").text = "My Company"
    etree.SubElement(shop, "url").text = "https://myshop.example.com"

    categories_el = etree.SubElement(shop, "categories")
    etree.SubElement(categories_el, "category", id="1").text = "–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è"

    offers_el = etree.SubElement(shop, "offers")
    for offer in offers:
        offers_el.append(offer)

    return etree.ElementTree(yml_catalog)

def save_yml_by_size(offers):
    file_index = 1
    current_offers = []

    for offer in offers:
        current_offers.append(offer)
        tree = build_prom_yml(current_offers)
        xml_bytes = etree.tostring(tree, encoding="utf-8", xml_declaration=True, pretty_print=True)

        if len(xml_bytes) >= MAX_FILE_SIZE_BYTES:
            current_offers.pop()  # –ó–∞–±–∏—Ä–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π, –±–æ –≤—ñ–Ω –ø–µ—Ä–µ–ø–æ–≤–Ω–∏–≤
            tree = build_prom_yml(current_offers)
            filename = f"output_{file_index}.yml"
            tree.write(filename, encoding="utf-8", xml_declaration=True, pretty_print=True)
            print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({len(current_offers)} —Ç–æ–≤–∞—Ä—ñ–≤)")

            file_index += 1
            current_offers = [offer]  # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ

    if current_offers:
        tree = build_prom_yml(current_offers)
        filename = f"output_{file_index}.yml"
        tree.write(filename, encoding="utf-8", xml_declaration=True, pretty_print=True)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({len(current_offers)} —Ç–æ–≤–∞—Ä—ñ–≤)")

def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")

    all_offers = []
    successful_feeds = 0
    failed_feeds = 0

    for url in urls:
        offers = fetch_offers_from_url(url)
        if offers:
            successful_feeds += 1
            all_offers.extend(offers)
        else:
            failed_feeds += 1

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {successful_feeds}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed_feeds}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {len(all_offers)}")

    save_yml_by_size(all_offers)

if __name__ == "__main__":
    main()
