import os
import aiohttp
import asyncio
from lxml import etree
from datetime import datetime
import hashlib
from io import BytesIO

FEEDS_FILE = "feeds.txt"
MAX_FILE_SIZE_MB = 100
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}


# -------------------- –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø URL --------------------
def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]


# -------------------- –ü–û–¢–û–ö–û–í–ò–ô –ü–ê–†–°–ò–ù–ì --------------------
def iter_offers(xml_bytes):
    try:
        context = etree.iterparse(BytesIO(xml_bytes), tag="offer", recover=True)
        for _, elem in context:
            # —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î–º–æ –æ—Ñ—Ñ–µ—Ä —É –±–∞–π—Ç–∏ –π –ø–∞—Ä—Å–∏–º–æ –∑–Ω–æ–≤—É,
            # —â–æ–± –≤—ñ–Ω –±—É–≤ –ø–æ–≤–Ω–∏–º —ñ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏–º –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
            offer_copy = etree.fromstring(etree.tostring(elem))
            yield offer_copy
            elem.clear()  # —á–∏—Å—Ç–∏–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É XML: {e}")


# -------------------- –ê–°–ò–ù–•–†–û–ù–ù–ï –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø --------------------
async def fetch_offers_from_url(session, url):
    try:
        async with session.get(url, headers=HEADERS, timeout=60) as response:
            if response.status != 200:
                print(f"‚ùå {url} ‚Äî HTTP {response.status}")
                return []
            content = await response.read()
            offers = list(iter_offers(content))
            print(f"‚úÖ {url} ‚Äî {len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers
    except Exception as e:
        print(f"‚ùå {url}: {e}")
        return []


async def fetch_all_offers(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_offers_from_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [offer for sublist in results for offer in sublist], results


# -------------------- –§–û–†–ú–£–í–ê–ù–ù–Ø YML --------------------
def build_prom_yml(offers):
    yml_catalog = etree.Element(
        "yml_catalog", date=datetime.now().strftime("%Y-%m-%d %H:%M")
    )
    shop = etree.SubElement(yml_catalog, "shop")

    etree.SubElement(shop, "name").text = "MyShop"
    etree.SubElement(shop, "company").text = "My Company"
    etree.SubElement(shop, "url").text = "https://myshop.example.com"

    categories_el = etree.SubElement(shop, "categories")
    etree.SubElement(categories_el, "category", id="1").text = "–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è"

    offers_el = etree.SubElement(shop, "offers")
    for offer in offers:
        offers_el.append(offer)

    return etree.ElementTree(yml_catalog)


# -------------------- –•–ï–® –§–ê–ô–õ–£ --------------------
def file_hash(path):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()


# -------------------- –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø YML --------------------
def save_file(offers, filename):
    tree = build_prom_yml(offers)
    xml_bytes = etree.tostring(tree, encoding="utf-8", xml_declaration=True)

    new_hash = hashlib.md5(xml_bytes).hexdigest()
    old_hash = file_hash(filename)

    if new_hash != old_hash:
        tree.write(filename, encoding="utf-8", xml_declaration=True)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤)")
    else:
        print(f"‚ö†Ô∏è –ë–µ–∑ –∑–º—ñ–Ω: {filename}")


def save_yml_by_size(offers):
    file_index = 1
    current_offers = []

    for offer in offers:
        current_offers.append(offer)
        tree = build_prom_yml(current_offers)
        xml_bytes = etree.tostring(tree, encoding="utf-8", xml_declaration=True)

        if len(xml_bytes) > MAX_FILE_SIZE_BYTES:
            # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –±–µ–∑ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –æ—Ñ—Ñ–µ—Ä–∞
            current_offers.pop()
            save_file(current_offers, f"output_{file_index}.yml")

            file_index += 1
            current_offers = [offer]

    if current_offers:
        save_file(current_offers, f"output_{file_index}.yml")


# -------------------- MAIN --------------------
def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")

    if not urls:
        return

    all_offers, results = asyncio.run(fetch_all_offers(urls))

    successful_feeds = sum(1 for r in results if r)
    failed_feeds = len(urls) - successful_feeds

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {successful_feeds}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed_feeds}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {len(all_offers)}")

    save_yml_by_size(all_offers)


if __name__ == "__main__":
    main()
