import os
import aiohttp
import asyncio
from lxml import etree
from datetime import datetime
from io import BytesIO
import hashlib
import re

FEEDS_FILE = "feeds.txt"
MAX_FILE_SIZE_MB = 95
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}

# -------------------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL --------------------
def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]

# -------------------- –°–∞–Ω—ñ—Ç–∞–π–∑ —Ç–µ–∫—Å—Ç—É --------------------
def escape_text(text):
    if not text:
        return text
    # –ó–∞–º—ñ–Ω—é—î–º–æ –Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ —Å–∏–º–≤–æ–ª–∏
    text = re.sub(r'&(?![a-zA-Z]+;|#\d+;)', '&amp;', text)
    text = text.replace('"', "'")  # –ª–∞–ø–∫–∏
    text = text.replace('<', '&lt;').replace('>', '&gt;')
    return text.strip()

def sanitize_offer(elem):
    for child in elem.iter():
        if child.text:
            child.text = escape_text(child.text)
        if child.tail:
            child.tail = escape_text(child.tail)
    return elem

# -------------------- –ü–æ—Ç–æ–∫–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ --------------------
def iter_offers(xml_bytes):
    try:
        context = etree.iterparse(BytesIO(xml_bytes), tag="offer", recover=True)
        for _, elem in context:
            yield sanitize_offer(elem)
            elem.clear()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É XML: {e}")

# -------------------- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è --------------------
async def fetch_offers_from_url(session, url):
    try:
        async with session.get(url, headers=HEADERS, timeout=120) as response:
            if response.status != 200:
                print(f"‚ùå {url} ‚Äî HTTP {response.status}")
                return {}
            content = await response.read()
            offers_dict = {}
            for offer in iter_offers(content):
                sku = offer.get("id") or offer.findtext("vendorCode")
                if not sku:
                    continue
                offers_dict[sku] = etree.tostring(offer, encoding="utf-8").decode("utf-8")
            print(f"‚úÖ {url} ‚Äî {len(offers_dict)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers_dict
    except Exception as e:
        print(f"‚ùå {url}: {e}")
        return {}

async def fetch_all_offers(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_offers_from_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        merged = {}
        for offers in results:
            merged.update(offers)  # –æ—Å—Ç–∞–Ω–Ω—ñ–π –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—î –¥—É–±–ª—å
        return merged, results

# -------------------- –•–µ—à —Ñ–∞–π–ª—É --------------------
def file_hash(path):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

# -------------------- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É –∫—ñ–ª—å–∫–∞ —Ñ–∞–π–ª—ñ–≤ --------------------
def save_split_yml(offers_dict):
    offers = list(offers_dict.values())

    header = '<?xml version="1.0" encoding="UTF-8"?>\n'
    header += f'<yml_catalog date="{datetime.now().strftime("%Y-%m-%d %H:%M")}">\n'
    header += "<shop>\n"
    header += "<name>MyShop</name>\n"
    header += "<company>My Company</company>\n"
    header += "<url>https://myshop.example.com</url>\n"
    header += '<categories><category id="1">–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è</category></categories>\n'
    header += "<offers>\n"

    footer = "</offers>\n</shop>\n</yml_catalog>\n"

    file_index = 1
    current_parts = [header]
    current_size = len(header.encode("utf-8"))
    current_count = 0

    for offer in offers:
        offer_bytes = (offer + "\n").encode("utf-8")

        if current_size + len(offer_bytes) + len(footer.encode("utf-8")) > MAX_FILE_SIZE_BYTES:
            current_parts.append(footer)
            xml_bytes = "".join(current_parts).encode("utf-8")

            filename = f"all_{file_index}.yml"
            new_hash = hashlib.md5(xml_bytes).hexdigest()
            old_hash = file_hash(filename)

            if new_hash != old_hash:
                with open(filename, "wb") as f:
                    f.write(xml_bytes)
                print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({current_count} —Ç–æ–≤–∞—Ä—ñ–≤)")
            else:
                print(f"‚ö†Ô∏è –ë–µ–∑ –∑–º—ñ–Ω: {filename}")

            file_index += 1
            current_parts = [header, offer + "\n"]
            current_size = len(header.encode("utf-8")) + len(offer_bytes)
            current_count = 1
        else:
            current_parts.append(offer + "\n")
            current_size += len(offer_bytes)
            current_count += 1

    if len(current_parts) > 1:
        current_parts.append(footer)
        xml_bytes = "".join(current_parts).encode("utf-8")

        filename = f"all_{file_index}.yml"
        new_hash = hashlib.md5(xml_bytes).hexdigest()
        old_hash = file_hash(filename)

        if new_hash != old_hash:
            with open(filename, "wb") as f:
                f.write(xml_bytes)
            print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({current_count} —Ç–æ–≤–∞—Ä—ñ–≤)")
        else:
            print(f"‚ö†Ô∏è –ë–µ–∑ –∑–º—ñ–Ω: {filename}")

# -------------------- MAIN --------------------
def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")

    if not urls:
        return

    all_offers_dict, results = asyncio.run(fetch_all_offers(urls))

    successful_feeds = sum(1 for r in results if r)
    failed_feeds = len(urls) - successful_feeds

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {successful_feeds}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed_feeds}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {len(all_offers_dict)}")

    save_split_yml(all_offers_dict)

if __name__ == "__main__":
    main()
