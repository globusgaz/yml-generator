import os
import aiohttp
import asyncio
from lxml import etree
from datetime import datetime

FEEDS_FILE = "feeds.txt"
OUTPUT_FILE = "all.yml"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}

# -------------------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL --------------------
def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]

# -------------------- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è --------------------
async def fetch_and_write_offers(session, url, f):
    try:
        async with session.get(url, headers=HEADERS, timeout=120) as response:
            if response.status != 200:
                print(f"‚ùå {url} ‚Äî HTTP {response.status}")
                return 0

            offers = 0
            context = etree.iterparse(await response.content.readany(), tag="offer", recover=True)
            # lxml –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î –Ω–∞–ø—Ä—è–º—É asyncio stream ‚Üí —á–∏—Ç–∞—î–º–æ chunks
            async for chunk in response.content.iter_chunked(1024 * 1024):
                for _, elem in etree.iterparse(
                    io.BytesIO(chunk), tag="offer", recover=True
                ):
                    f.write(etree.tostring(elem, encoding="utf-8").decode("utf-8"))
                    offers += 1
                    elem.clear()

            print(f"‚úÖ {url} ‚Äî {offers} —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers
    except Exception as e:
        print(f"‚ùå {url}: {e}")
        return 0

async def process_feeds(urls, f):
    total_offers = 0
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_and_write_offers(session, url, f) for url in urls]
        results = await asyncio.gather(*tasks)
        total_offers = sum(results)
    return total_offers, len([r for r in results if r > 0]), len([r for r in results if r == 0])

# -------------------- MAIN --------------------
def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")
    if not urls:
        return

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        f.write(f'<yml_catalog date="{datetime.now().strftime("%Y-%m-%d %H:%M")}">\n')
        f.write("<shop>\n")
        f.write("<name>MyShop</name>\n")
        f.write("<company>My Company</company>\n")
        f.write("<url>https://myshop.example.com</url>\n")
        f.write('<categories><category id="1">–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è</category></categories>\n')
        f.write("<offers>\n")

        total_offers, ok, failed = asyncio.run(process_feeds(urls, f))

        # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        f.write("</offers>\n</shop>\n</yml_catalog>\n")

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {ok}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {total_offers}")
    print(f"üìÑ –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
