import os
import aiohttp
import asyncio
from lxml import etree
from datetime import datetime
import hashlib
from io import BytesIO

FEEDS_FILE = "feeds.txt"
MAX_FILE_SIZE_MB = 100
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}

# -------------------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL --------------------
def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]

# -------------------- –ü–æ—Ç–æ–∫–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ --------------------
def iter_offers(xml_bytes):
    try:
        context = etree.iterparse(BytesIO(xml_bytes), tag="offer", recover=True)
        for _, elem in context:
            yield etree.tostring(elem, encoding="utf-8").decode("utf-8")
            elem.clear()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É XML: {e}")

# -------------------- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è --------------------
async def fetch_offers_from_url(session, url):
    try:
        async with session.get(url, headers=HEADERS, timeout=60) as response:
            if response.status != 200:
                print(f"‚ùå {url} ‚Äî HTTP {response.status}")
                return []
            content = await response.read()
            offers = list(iter_offers(content))
            print(f"‚úÖ {url} ‚Äî {len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers
    except Exception as e:
        print(f"‚ùå {url}: {e}")
        return []

async def fetch_all_offers(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_offers_from_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [offer for sublist in results for offer in sublist], results

# -------------------- –§–æ—Ä–º—É–≤–∞–Ω–Ω—è YML --------------------
def build_yml_string(offers_str_list):
    parts = []
    parts.append('<?xml version="1.0" encoding="UTF-8"?>')
    parts.append(f'<yml_catalog date="{datetime.now().strftime("%Y-%m-%d %H:%M")}">')
    parts.append("<shop>")
    parts.append("<name>MyShop</name>")
    parts.append("<company>My Company</company>")
    parts.append("<url>https://myshop.example.com</url>")
    parts.append('<categories><category id="1">–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è</category></categories>')
    parts.append("<offers>")
    parts.extend(offers_str_list)  # –≤–∂–µ –≥–æ—Ç–æ–≤—ñ —Ä—è–¥–∫–∏ XML
    parts.append("</offers>")
    parts.append("</shop>")
    parts.append("</yml_catalog>")
    return "\n".join(parts).encode("utf-8")

# -------------------- –•–µ—à —Ñ–∞–π–ª—É --------------------
def file_hash(path):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

# -------------------- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è YML --------------------
def save_file(offers, filename):
    xml_bytes = build_yml_string(offers)
    new_hash = hashlib.md5(xml_bytes).hexdigest()
    old_hash = file_hash(filename)

    if new_hash != old_hash:
        with open(filename, "wb") as f:
            f.write(xml_bytes)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤)")
    else:
        print(f"‚ö†Ô∏è –ë–µ–∑ –∑–º—ñ–Ω: {filename}")

def save_yml_by_size(offers):
    file_index = 1
    current_offers = []

    for offer in offers:
        current_offers.append(offer)
        xml_bytes = build_yml_string(current_offers)

        if len(xml_bytes) > MAX_FILE_SIZE_BYTES:
            current_offers.pop()
            save_file(current_offers, f"output_{file_index}.yml")
            file_index += 1
            current_offers = [offer]

    if current_offers:
        save_file(current_offers, f"output_{file_index}.yml")

# -------------------- MAIN --------------------
def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")

    if not urls:
        return

    all_offers, results = asyncio.run(fetch_all_offers(urls))

    successful_feeds = sum(1 for r in results if r)
    failed_feeds = len(urls) - successful_feeds

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {successful_feeds}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed_feeds}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {len(all_offers)}")

    save_yml_by_size(all_offers)

if __name__ == "__main__":
    main()
