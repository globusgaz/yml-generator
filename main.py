import os
import aiohttp
import asyncio
from lxml import etree
from datetime import datetime
import hashlib
import tempfile

FEEDS_FILE = "feeds.txt"
MAX_FILE_SIZE_MB = 100
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; YML-Generator/1.0)"}


# -------------------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL --------------------
def load_urls():
    if not os.path.exists(FEEDS_FILE):
        print(f"‚ùå –§–∞–π–ª {FEEDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return []
    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip().startswith("http")]


# -------------------- –ü–æ—Ç–æ–∫–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ --------------------
def iter_offers_from_stream(stream):
    context = etree.iterparse(stream, tag="offer", recover=True)
    for _, elem in context:
        yield etree.tostring(elem, encoding="utf-8")
        elem.clear()


# -------------------- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è --------------------
async def fetch_and_parse(session, url):
    offers = []
    try:
        async with session.get(url, headers=HEADERS, timeout=180) as resp:
            if resp.status != 200:
                print(f"‚ùå {url} ‚Äî HTTP {resp.status}")
                return []

            # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª, —â–æ–± –≤—ñ–¥–¥–∞—Ç–∏ iterparse
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                while True:
                    chunk = await resp.content.read(65536)
                    if not chunk:
                        break
                    tmp.write(chunk)
                tmp_path = tmp.name

            with open(tmp_path, "rb") as f:
                for offer in iter_offers_from_stream(f):
                    offers.append(offer)

            os.remove(tmp_path)
            print(f"‚úÖ {url} ‚Äî {len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers

    except Exception as e:
        print(f"‚ùå {url}: {e}")
        return []


async def fetch_all_offers(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_and_parse(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [offer for sublist in results for offer in sublist], results


# -------------------- –•–µ—à --------------------
def file_hash(path):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()


# -------------------- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ—Ç–æ–∫–æ–≤–µ --------------------
def save_yml_stream(offers):
    file_index = 1
    size = 0
    outfile = None

    def start_file(idx):
        path = f"output_{idx}.yml"
        f = open(path, "wb")
        f.write(f'<?xml version="1.0" encoding="UTF-8"?>\n'.encode())
        f.write(f'<yml_catalog date="{datetime.now().strftime("%Y-%m-%d %H:%M")}">\n'.encode())
        f.write(b"<shop>\n")
        f.write(b"<name>MyShop</name>\n")
        f.write(b"<company>My Company</company>\n")
        f.write(b"<url>https://myshop.example.com</url>\n")
        f.write(b'<categories><category id="1">–ó–∞–≥–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è</category></categories>\n')
        f.write(b"<offers>\n")
        return f, path

    def end_file(f):
        f.write(b"</offers>\n</shop>\n</yml_catalog>\n")
        f.close()

    outfile, path = start_file(file_index)

    for offer in offers:
        encoded = offer + b"\n"
        if size + len(encoded) > MAX_FILE_SIZE_BYTES:
            end_file(outfile)
            print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {path}")
            file_index += 1
            size = 0
            outfile, path = start_file(file_index)
        outfile.write(encoded)
        size += len(encoded)

    if outfile:
        end_file(outfile)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {path}")


# -------------------- MAIN --------------------
def main():
    urls = load_urls()
    print(f"\nüîó –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} –ø–æ—Å–∏–ª–∞–Ω—å —É {FEEDS_FILE}\n")
    if not urls:
        return

    all_offers, results = asyncio.run(fetch_all_offers(urls))

    successful_feeds = sum(1 for r in results if r)
    failed_feeds = len(urls) - successful_feeds

    print("\nüìä –ü—ñ–¥—Å—É–º–æ–∫:")
    print(f"üîπ –í—Å—å–æ–≥–æ —Ñ—ñ–¥—ñ–≤: {len(urls)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {successful_feeds}")
    print(f"‚ùå –ó –ø–æ–º–∏–ª–∫–∞–º–∏: {failed_feeds}")
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {len(all_offers)}")

    save_yml_stream(all_offers)


if __name__ == "__main__":
    main()
